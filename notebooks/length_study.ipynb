{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 11:38:34.192968: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 11:38:34.927809: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 11:38:34.927889: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 11:38:34.927894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertTokenizer,\n",
    "    BartTokenizer,\n",
    "    T5Tokenizer,\n",
    "    GPT2Tokenizer,\n",
    ")\n",
    "from typing import Union, List, Dict, Tuple\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import List, Optional\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.utils.logging import get_logger\n",
    "\n",
    "\n",
    "class DGDataset:\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: str = \"blended_skill_talk\",\n",
    "        task: str = \"seq2seq\",\n",
    "        tokenizer: AutoTokenizer = None,\n",
    "        max_source_length: int = 512,\n",
    "        max_target_length: int = 512,\n",
    "        padding: str = \"max_length\",\n",
    "        ignore_pad_token_for_loss: bool = True,\n",
    "        preprocessing_num_workers: int = None,\n",
    "        overwrite_cache: bool = True,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.task = task\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.padding = padding\n",
    "        self.ignore_pad_token_for_loss = ignore_pad_token_for_loss\n",
    "        self.preprocessing_num_workers = preprocessing_num_workers\n",
    "        self.overwrite_cache = overwrite_cache\n",
    "        # since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "        self.tok_logger = get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "\n",
    "    def prepare_context(self, instance: dict):\n",
    "        if self.dataset == 'blended_skill_talk':\n",
    "            num_entries = len(instance[\"free_messages\"])\n",
    "            total_entries = num_entries\n",
    "            if self.task == 'seq2seq':\n",
    "                persona_pieces = f\"<PS>{instance['personas'][1]}\"\n",
    "                if instance['context'] == \"wizard_of_wikipedia\":\n",
    "                    additional_context_pieces = f\"<CTX>{instance['additional_context']}.\"\n",
    "                else:\n",
    "                    additional_context_pieces = \"\"\n",
    "                context = persona_pieces + additional_context_pieces\n",
    "            else:\n",
    "                num_entries = min(num_entries, 2)\n",
    "                context = ''\n",
    "            prev_utt_pc = [sent for sent in instance[\"previous_utterance\"] if sent != '']\n",
    "\n",
    "        elif self.dataset == 'conv_ai_2':\n",
    "            total_entries = len(instance['dialog'])\n",
    "            num_entries = total_entries//2\n",
    "            if self.task == 'seq2seq':\n",
    "                user_profile = ' '.join([''.join(x) for x in instance['user_profile']])\n",
    "                persona_pieces = f\"<PS>{user_profile}\"\n",
    "                context = persona_pieces\n",
    "            else:\n",
    "                num_entries = min(num_entries, 2)\n",
    "                context = ''\n",
    "            prev_utt_pc = []\n",
    "\n",
    "        elif self.dataset == 'empathetic_dialogues':\n",
    "            total_entries = len(instance['dialog'])\n",
    "            num_entries = total_entries//2\n",
    "            if self.task == 'seq2seq':\n",
    "                persona_pieces = f\"<PS>{instance['prompt']}\"\n",
    "                additional_context_pieces = f\"<CTX>{instance['context']}.\"\n",
    "                context = persona_pieces + additional_context_pieces\n",
    "            else:\n",
    "                num_entries = min(num_entries, 2)\n",
    "                context = ''\n",
    "            prev_utt_pc = []\n",
    "\n",
    "        elif self.dataset == 'AlekseyKorshuk/persona-chat':\n",
    "            total_entries = len(instance['utterances'])\n",
    "            num_entries = total_entries//2\n",
    "            if self.task == 'seq2seq':\n",
    "                user_profile = ' '.join(instance['personality'])\n",
    "                persona_pieces = f\"<PS>{user_profile}\"\n",
    "                context = persona_pieces\n",
    "            else:\n",
    "                num_entries = min(num_entries, 2)\n",
    "                context = ''\n",
    "            prev_utt_pc = []\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Dataset not supported.\")\n",
    "        return num_entries, total_entries, context, prev_utt_pc\n",
    "\n",
    "\n",
    "    def prepare_entry(\n",
    "        self, \n",
    "        instance: dict, \n",
    "        entry_idx: int, \n",
    "        context: str, \n",
    "        prev_utt_pc: List[str], \n",
    "        total_entries: int,\n",
    "    ):\n",
    "        if self.dataset == 'blended_skill_talk':\n",
    "            free_message = instance['free_messages'][entry_idx]\n",
    "            guided_message = instance['guided_messages'][entry_idx]\n",
    "            references = [values[entry_idx] for key, values in instance['suggestions'].items()]\n",
    "\n",
    "        elif self.dataset == 'conv_ai_2':\n",
    "            free_message = instance['dialog'][entry_idx*2]['text']\n",
    "            if entry_idx*2+1 >= total_entries:\n",
    "                guided_message = None\n",
    "            else:\n",
    "                guided_message = instance['dialog'][entry_idx*2+1]['text']\n",
    "            references = []\n",
    "\n",
    "        elif self.dataset == 'empathetic_dialogues':\n",
    "            free_message = instance['dialog'][entry_idx*2]['text']\n",
    "            if entry_idx*2+1 >= total_entries:\n",
    "                guided_message = None\n",
    "            else:\n",
    "                guided_message = instance['dialog'][entry_idx*2+1]['text']\n",
    "            references = []\n",
    "\n",
    "        elif self.dataset == 'AlekseyKorshuk/persona-chat':\n",
    "            free_message = instance['utterances'][entry_idx*2]['history'][-1]\n",
    "            if entry_idx*2+1 >= total_entries:\n",
    "                guided_message = None\n",
    "            else:\n",
    "                guided_message = instance['utterances'][entry_idx*2+1]['history'][-1]\n",
    "            references = instance['utterances'][entry_idx*2]['candidates']\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Dataset not supported.\")\n",
    "\n",
    "        if not prev_utt_pc:\n",
    "            original_context = context\n",
    "        else:\n",
    "            sp_token = '<SEP>' if self.task == 'seq2seq' else ' '\n",
    "            original_context = context + sp_token + sp_token.join(prev_utt_pc)\n",
    "        \n",
    "        references.append(guided_message)\n",
    "        return free_message, guided_message, original_context, references\n",
    "\n",
    "\n",
    "    def tokenize_and_align_labels(self, instance: dict):\n",
    "        num_entries, total_entries, context, prev_utt_pc = self.prepare_context(instance)\n",
    "        inputs, labels = [], []\n",
    "        for entry_idx in range(num_entries):\n",
    "            free_message, guided_message, original_context, references = self.prepare_entry(\n",
    "                instance, \n",
    "                entry_idx, \n",
    "                context, \n",
    "                prev_utt_pc,\n",
    "                total_entries,\n",
    "            )\n",
    "            if guided_message is None:\n",
    "                continue\n",
    "            # Input & Output\n",
    "            if self.task == 'seq2seq':\n",
    "                text = original_context + self.tokenizer.eos_token + free_message\n",
    "            else:\n",
    "                text = original_context + free_message + guided_message\n",
    "\n",
    "            inputs.append(text)\n",
    "            labels.append(guided_message)\n",
    "            prev_utt_pc += [\n",
    "                free_message,\n",
    "                guided_message,\n",
    "            ]\n",
    "        \n",
    "        if not inputs:\n",
    "            return {\"input_ids\": [], \"labels\": [], \"attention_mask\": []}\n",
    "\n",
    "        if self.task == 'seq2seq':\n",
    "            inputs = self.tokenizer(inputs, max_length=self.max_source_length, padding=self.padding, truncation=True)\n",
    "            # Setup the tokenizer for targets\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                labels = self.tokenizer(labels, max_length=self.max_target_length, padding=self.padding, truncation=True)\n",
    "            \n",
    "            # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
    "            # when we want to ignore padding in the loss.\n",
    "            if self.padding == \"max_length\" and self.ignore_pad_token_for_loss:\n",
    "                labels[\"input_ids\"] = [\n",
    "                    [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "                ]\n",
    "            inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return inputs\n",
    "        else:\n",
    "            with CaptureLogger(self.tok_logger) as cl:\n",
    "                inputs = self.tokenizer(\n",
    "                    inputs, \n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=self.max_source_length, \n",
    "                    padding=self.padding, \n",
    "                    truncation=True,\n",
    "                )\n",
    "                labels = self.tokenizer(\n",
    "                    labels, \n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=self.max_target_length, \n",
    "                    padding=self.padding, \n",
    "                    truncation=True,\n",
    "                )\n",
    "                \n",
    "            new_inputs = inputs.copy()\n",
    "            for k, v1 in inputs.items():\n",
    "                v2 = labels[k]\n",
    "                new_inputs[k] = torch.cat((v1, v2), dim=1)\n",
    "                \n",
    "            new_labels = torch.cat((-100*torch.ones_like(inputs[\"input_ids\"]), labels[\"input_ids\"]), dim=1)\n",
    "            new_inputs[\"labels\"] = new_labels\n",
    "\n",
    "            # clm input could be much much longer than block_size\n",
    "            if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "                self.tok_logger.warning(\n",
    "                    \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "                    \" before being passed to the model.\"\n",
    "                )\n",
    "            return new_inputs\n",
    "\n",
    "\n",
    "    def group_texts(self, examples):\n",
    "        # ['input_ids', 'attention_mask', 'labels']\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        return concatenated_examples\n",
    "\n",
    "\n",
    "    def group_ED(self, dataset: Dataset):\n",
    "        results = {\n",
    "            'conv_id': [], \n",
    "            'prompt': [],\n",
    "            'dialog': [], \n",
    "            'context': [],\n",
    "        }\n",
    "        for i, instance in enumerate(dataset):\n",
    "            if instance['utterance_idx'] == 1:\n",
    "                results['conv_id'].append(instance['conv_id'])\n",
    "                results['dialog'].append([])\n",
    "                results['prompt'].append(instance['prompt'])\n",
    "                results['context'].append(instance['context'])\n",
    "\n",
    "            response = {'text': instance['utterance'], 'speaker_idx': instance['speaker_idx']}\n",
    "            results['dialog'][-1].append(response)\n",
    "        return Dataset.from_dict(results)\n",
    "\n",
    "\n",
    "    def preprocess(self, dataset: Dataset):\n",
    "        if self.dataset == \"empathetic_dialogues\":\n",
    "            dataset = self.group_ED(dataset)\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            self.tokenize_and_align_labels,\n",
    "            batched=False,\n",
    "            num_proc=self.preprocessing_num_workers,\n",
    "            remove_columns=dataset.column_names,\n",
    "            load_from_cache_file=not self.overwrite_cache,\n",
    "        )\n",
    "        dataset = dataset.map(\n",
    "            self.group_texts,\n",
    "            batched=True,\n",
    "            num_proc=self.preprocessing_num_workers,\n",
    "            load_from_cache_file=not self.overwrite_cache,\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset blended_skill_talk (/home/yuli/.cache/huggingface/datasets/blended_skill_talk/default/1.0.0/8544e13cbbf2fb9b34157f2e2f28c1539e4f36bf0ef2bd96edd138b4000c5ca1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5c52f3ce7445a484c4b892c3468342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = '../../DG_ckpt/bart'\n",
    "data_n = 'blended_skill_talk'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "sp_token = tokenizer.eos_token\n",
    "datasets = load_dataset(data_n)\n",
    "train_dataset = datasets['train']\n",
    "val_dataset = datasets['validation']\n",
    "test_dataset = datasets['test']\n",
    "dg = DGDataset(\n",
    "    dataset=\"blended_skill_talk\",\n",
    "    task=\"seq2seq\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4819it [00:47, 102.24it/s]\n",
      "1009it [00:10, 95.11it/s]\n",
      "980it [00:10, 93.56it/s] \n"
     ]
    }
   ],
   "source": [
    "def preprocess(\n",
    "    dataset: Dataset, \n",
    "    tokenizer: Union[BertTokenizer, BartTokenizer, T5Tokenizer, GPT2Tokenizer],\n",
    "): \n",
    "    processed = []\n",
    "    for i, ins in tqdm(enumerate(dataset)):\n",
    "        num_entries, total_entries, context, prev_utt_pc = dg.prepare_context(ins)\n",
    "        for entry_idx in range(num_entries):\n",
    "            free_message, guided_message, original_context, references = dg.prepare_entry(\n",
    "                ins, \n",
    "                entry_idx, \n",
    "                context, \n",
    "                prev_utt_pc,\n",
    "                total_entries,\n",
    "            )\n",
    "            if guided_message is None:\n",
    "                continue\n",
    "            \n",
    "            prev_utt_pc += [\n",
    "                free_message,\n",
    "                guided_message,\n",
    "            ]\n",
    "            \n",
    "            # Original generation\n",
    "            text = original_context + sp_token + free_message\n",
    "            for ref in references:\n",
    "                processed.append({\n",
    "                    'src': text,\n",
    "                    'tgt': ref,\n",
    "                    'src_len': len(tokenizer.tokenize(text)),\n",
    "                    'tgt_len': len(tokenizer.tokenize(ref)),\n",
    "                })\n",
    "            # processed.append({\n",
    "            #     'input': text,\n",
    "            #     'references': references,\n",
    "            #     'input_length': len(tokenizer.tokenize(text)),\n",
    "            #     'references_length': [len(tokenizer.tokenize(ref)) for ref in references],\n",
    "            # })\n",
    "    processed = pd.DataFrame(processed, columns=['src', 'tgt', 'src_len', 'tgt_len'])\n",
    "    return processed\n",
    "\n",
    "data_dir = '../datasets'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "processed_train = preprocess(train_dataset, tokenizer)\n",
    "processed_val = preprocess(val_dataset, tokenizer)\n",
    "processed_test = preprocess(test_dataset, tokenizer)\n",
    "processed_train.to_csv(f'{data_dir}/train.tsv', sep='\\t', index=False)\n",
    "processed_val.to_csv(f'{data_dir}/val.tsv', sep='\\t', index=False)\n",
    "processed_test.to_csv(f'{data_dir}/dev.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(dataset: pd.DataFrame, column: str = 'src'):\n",
    "    for i, row in dataset.iterrows():\n",
    "        yield row[column].strip().split()\n",
    "\n",
    "src_vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(processed_train, 'src'), \n",
    "    specials=['<PS>', '<SEP>', '</s>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['src', 'tgt', 'src_len', 'tgt_len'],\n",
       "    num_rows: 108072\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_csv(f'{data_dir}/train.tsv', sep='\\t')\n",
    "t = Dataset.from_pandas(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': \"<PS>i love flowers.<SEP>I love live music, that's why I try to go to concerts<SEP>I do too. Wat do you like?</s>I like acting, I hope to be an actor, what about you?\",\n",
       " 'tgt': \"i love acting ! i'll be famous someday . what do you do ?\",\n",
       " 'src_len': 46,\n",
       " 'tgt_len': 15}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = \"i love acting ! i'll be famous someday . what do you do ?\"\n",
    "inputs = tokenizer(tgt, return_tensors=\"pt\", max_length=128, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i love acting! i'll be famous someday. what do you do?\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I'm thirsty now</th>\n",
       "      <td>[[1, 4], 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           b\n",
       "a                           \n",
       "I'm thirsty now  [[1, 4], 4]\n",
       "good                     [3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [{'a': \"I'm thirsty now\", 'b': [1,4]}, {'a': 'good', 'b': 3}, {'a': \"I'm thirsty now\", 'b': 4}]\n",
    "pd.DataFrame(x).groupby('a').agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
