{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 16:52:19.848632: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 16:52:20.695368: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 16:52:20.695445: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 16:52:20.695456: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertTokenizer,\n",
    "    BartTokenizer,\n",
    "    T5Tokenizer,\n",
    "    GPT2Tokenizer,\n",
    ")\n",
    "from typing import Union, List, Dict, Tuple\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import List, Optional\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.utils.logging import get_logger\n",
    "\n",
    "\n",
    "class DGDataset:\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: str = \"blended_skill_talk\",\n",
    "        task: str = \"seq2seq\",\n",
    "        tokenizer: AutoTokenizer = None,\n",
    "        max_source_length: int = 512,\n",
    "        max_target_length: int = 512,\n",
    "        padding: str = \"max_length\",\n",
    "        ignore_pad_token_for_loss: bool = True,\n",
    "        preprocessing_num_workers: int = None,\n",
    "        overwrite_cache: bool = True,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.task = task\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.padding = padding\n",
    "        self.ignore_pad_token_for_loss = ignore_pad_token_for_loss\n",
    "        self.preprocessing_num_workers = preprocessing_num_workers\n",
    "        self.overwrite_cache = overwrite_cache\n",
    "        # since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "        self.tok_logger = get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "\n",
    "    def prepare_context(self, instance: dict):\n",
    "        if self.dataset == 'blended_skill_talk':\n",
    "            num_entries = len(instance[\"free_messages\"])\n",
    "            total_entries = num_entries\n",
    "            if self.task == 'seq2seq':\n",
    "                persona_pieces = f\"<PS>{instance['personas'][1]}\"\n",
    "                if instance['context'] == \"wizard_of_wikipedia\":\n",
    "                    additional_context_pieces = f\"<CTX>{instance['additional_context']}.\"\n",
    "                else:\n",
    "                    additional_context_pieces = \"\"\n",
    "                context = persona_pieces + additional_context_pieces\n",
    "            else:\n",
    "                num_entries = min(num_entries, 2)\n",
    "                context = ''\n",
    "            prev_utt_pc = [sent for sent in instance[\"previous_utterance\"] if sent != '']\n",
    "\n",
    "        elif self.dataset == 'conv_ai_2':\n",
    "            total_entries = len(instance['dialog'])\n",
    "            num_entries = total_entries//2\n",
    "            if self.task == 'seq2seq':\n",
    "                user_profile = ' '.join([''.join(x) for x in instance['user_profile']])\n",
    "                persona_pieces = f\"<PS>{user_profile}\"\n",
    "                context = persona_pieces\n",
    "            else:\n",
    "                num_entries = min(num_entries, 2)\n",
    "                context = ''\n",
    "            prev_utt_pc = []\n",
    "\n",
    "        elif self.dataset == 'empathetic_dialogues':\n",
    "            total_entries = len(instance['dialog'])\n",
    "            num_entries = total_entries//2\n",
    "            if self.task == 'seq2seq':\n",
    "                persona_pieces = f\"<PS>{instance['prompt']}\"\n",
    "                additional_context_pieces = f\"<CTX>{instance['context']}.\"\n",
    "                context = persona_pieces + additional_context_pieces\n",
    "            else:\n",
    "                num_entries = min(num_entries, 2)\n",
    "                context = ''\n",
    "            prev_utt_pc = []\n",
    "\n",
    "        elif self.dataset == 'AlekseyKorshuk/persona-chat':\n",
    "            total_entries = len(instance['utterances'])\n",
    "            num_entries = total_entries//2\n",
    "            if self.task == 'seq2seq':\n",
    "                user_profile = ' '.join(instance['personality'])\n",
    "                persona_pieces = f\"<PS>{user_profile}\"\n",
    "                context = persona_pieces\n",
    "            else:\n",
    "                num_entries = min(num_entries, 2)\n",
    "                context = ''\n",
    "            prev_utt_pc = []\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Dataset not supported.\")\n",
    "        return num_entries, total_entries, context, prev_utt_pc\n",
    "\n",
    "\n",
    "    def prepare_entry(\n",
    "        self, \n",
    "        instance: dict, \n",
    "        entry_idx: int, \n",
    "        context: str, \n",
    "        prev_utt_pc: List[str], \n",
    "        total_entries: int,\n",
    "    ):\n",
    "        if self.dataset == 'blended_skill_talk':\n",
    "            free_message = instance['free_messages'][entry_idx]\n",
    "            guided_message = instance['guided_messages'][entry_idx]\n",
    "            references = [values[entry_idx] for key, values in instance['suggestions'].items()]\n",
    "\n",
    "        elif self.dataset == 'conv_ai_2':\n",
    "            free_message = instance['dialog'][entry_idx*2]['text']\n",
    "            if entry_idx*2+1 >= total_entries:\n",
    "                guided_message = None\n",
    "            else:\n",
    "                guided_message = instance['dialog'][entry_idx*2+1]['text']\n",
    "            references = []\n",
    "\n",
    "        elif self.dataset == 'empathetic_dialogues':\n",
    "            free_message = instance['dialog'][entry_idx*2]['text']\n",
    "            if entry_idx*2+1 >= total_entries:\n",
    "                guided_message = None\n",
    "            else:\n",
    "                guided_message = instance['dialog'][entry_idx*2+1]['text']\n",
    "            references = []\n",
    "\n",
    "        elif self.dataset == 'AlekseyKorshuk/persona-chat':\n",
    "            free_message = instance['utterances'][entry_idx*2]['history'][-1]\n",
    "            if entry_idx*2+1 >= total_entries:\n",
    "                guided_message = None\n",
    "            else:\n",
    "                guided_message = instance['utterances'][entry_idx*2+1]['history'][-1]\n",
    "            references = instance['utterances'][entry_idx*2]['candidates']\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Dataset not supported.\")\n",
    "\n",
    "        if not prev_utt_pc:\n",
    "            original_context = context\n",
    "        else:\n",
    "            sp_token = '<SEP>' if self.task == 'seq2seq' else ' '\n",
    "            original_context = context + sp_token + sp_token.join(prev_utt_pc)\n",
    "        \n",
    "        references.append(guided_message)\n",
    "        return free_message, guided_message, original_context, references\n",
    "\n",
    "\n",
    "    def tokenize_and_align_labels(self, instance: dict):\n",
    "        num_entries, total_entries, context, prev_utt_pc = self.prepare_context(instance)\n",
    "        inputs, labels = [], []\n",
    "        for entry_idx in range(num_entries):\n",
    "            free_message, guided_message, original_context, references = self.prepare_entry(\n",
    "                instance, \n",
    "                entry_idx, \n",
    "                context, \n",
    "                prev_utt_pc,\n",
    "                total_entries,\n",
    "            )\n",
    "            if guided_message is None:\n",
    "                continue\n",
    "            # Input & Output\n",
    "            if self.task == 'seq2seq':\n",
    "                text = original_context + self.tokenizer.eos_token + free_message\n",
    "            else:\n",
    "                text = original_context + free_message + guided_message\n",
    "\n",
    "            inputs.append(text)\n",
    "            labels.append(guided_message)\n",
    "            prev_utt_pc += [\n",
    "                free_message,\n",
    "                guided_message,\n",
    "            ]\n",
    "        \n",
    "        if not inputs:\n",
    "            return {\"input_ids\": [], \"labels\": [], \"attention_mask\": []}\n",
    "\n",
    "        if self.task == 'seq2seq':\n",
    "            inputs = self.tokenizer(inputs, max_length=self.max_source_length, padding=self.padding, truncation=True)\n",
    "            # Setup the tokenizer for targets\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                labels = self.tokenizer(labels, max_length=self.max_target_length, padding=self.padding, truncation=True)\n",
    "            \n",
    "            # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
    "            # when we want to ignore padding in the loss.\n",
    "            if self.padding == \"max_length\" and self.ignore_pad_token_for_loss:\n",
    "                labels[\"input_ids\"] = [\n",
    "                    [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "                ]\n",
    "            inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return inputs\n",
    "        else:\n",
    "            with CaptureLogger(self.tok_logger) as cl:\n",
    "                inputs = self.tokenizer(\n",
    "                    inputs, \n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=self.max_source_length, \n",
    "                    padding=self.padding, \n",
    "                    truncation=True,\n",
    "                )\n",
    "                labels = self.tokenizer(\n",
    "                    labels, \n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=self.max_target_length, \n",
    "                    padding=self.padding, \n",
    "                    truncation=True,\n",
    "                )\n",
    "                \n",
    "            new_inputs = inputs.copy()\n",
    "            for k, v1 in inputs.items():\n",
    "                v2 = labels[k]\n",
    "                new_inputs[k] = torch.cat((v1, v2), dim=1)\n",
    "                \n",
    "            new_labels = torch.cat((-100*torch.ones_like(inputs[\"input_ids\"]), labels[\"input_ids\"]), dim=1)\n",
    "            new_inputs[\"labels\"] = new_labels\n",
    "\n",
    "            # clm input could be much much longer than block_size\n",
    "            if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "                self.tok_logger.warning(\n",
    "                    \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "                    \" before being passed to the model.\"\n",
    "                )\n",
    "            return new_inputs\n",
    "\n",
    "\n",
    "    def group_texts(self, examples):\n",
    "        # ['input_ids', 'attention_mask', 'labels']\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        return concatenated_examples\n",
    "\n",
    "\n",
    "    def group_ED(self, dataset: Dataset):\n",
    "        results = {\n",
    "            'conv_id': [], \n",
    "            'prompt': [],\n",
    "            'dialog': [], \n",
    "            'context': [],\n",
    "        }\n",
    "        for i, instance in enumerate(dataset):\n",
    "            if instance['utterance_idx'] == 1:\n",
    "                results['conv_id'].append(instance['conv_id'])\n",
    "                results['dialog'].append([])\n",
    "                results['prompt'].append(instance['prompt'])\n",
    "                results['context'].append(instance['context'])\n",
    "\n",
    "            response = {'text': instance['utterance'], 'speaker_idx': instance['speaker_idx']}\n",
    "            results['dialog'][-1].append(response)\n",
    "        return Dataset.from_dict(results)\n",
    "\n",
    "\n",
    "    def preprocess(self, dataset: Dataset):\n",
    "        if self.dataset == \"empathetic_dialogues\":\n",
    "            dataset = self.group_ED(dataset)\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            self.tokenize_and_align_labels,\n",
    "            batched=False,\n",
    "            num_proc=self.preprocessing_num_workers,\n",
    "            remove_columns=dataset.column_names,\n",
    "            load_from_cache_file=not self.overwrite_cache,\n",
    "        )\n",
    "        dataset = dataset.map(\n",
    "            self.group_texts,\n",
    "            batched=True,\n",
    "            num_proc=self.preprocessing_num_workers,\n",
    "            load_from_cache_file=not self.overwrite_cache,\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset blended_skill_talk (/home/yuli/.cache/huggingface/datasets/blended_skill_talk/default/1.0.0/8544e13cbbf2fb9b34157f2e2f28c1539e4f36bf0ef2bd96edd138b4000c5ca1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5104dbbb344dcbb72d4afdbfa464b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = '../../DG_ckpt/bart'\n",
    "data_n = 'blended_skill_talk'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "sp_token = tokenizer.eos_token\n",
    "datasets = load_dataset(data_n)\n",
    "train_dataset = datasets['train']\n",
    "val_dataset = datasets['validation']\n",
    "test_dataset = datasets['test']\n",
    "dg = DGDataset(\n",
    "    dataset=\"blended_skill_talk\",\n",
    "    task=\"seq2seq\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4819it [00:48, 99.75it/s] \n",
      "1009it [00:10, 92.28it/s]\n",
      "980it [00:10, 90.73it/s] \n"
     ]
    }
   ],
   "source": [
    "def preprocess(\n",
    "    dataset: Dataset, \n",
    "    tokenizer: Union[BertTokenizer, BartTokenizer, T5Tokenizer, GPT2Tokenizer],\n",
    "): \n",
    "    processed = []\n",
    "    for i, ins in tqdm(enumerate(dataset)):\n",
    "        num_entries, total_entries, context, prev_utt_pc = dg.prepare_context(ins)\n",
    "        for entry_idx in range(num_entries):\n",
    "            free_message, guided_message, original_context, references = dg.prepare_entry(\n",
    "                ins, \n",
    "                entry_idx, \n",
    "                context, \n",
    "                prev_utt_pc,\n",
    "                total_entries,\n",
    "            )\n",
    "            if guided_message is None:\n",
    "                continue\n",
    "            \n",
    "            prev_utt_pc += [\n",
    "                free_message,\n",
    "                guided_message,\n",
    "            ]\n",
    "            \n",
    "            # Original generation\n",
    "            text = original_context + sp_token + free_message\n",
    "            for ref in references:\n",
    "                processed.append({\n",
    "                    'src': text,\n",
    "                    'tgt': ref,\n",
    "                    'src_len': len(tokenizer.tokenize(text)),\n",
    "                    'tgt_len': len(tokenizer.tokenize(ref)),\n",
    "                })\n",
    "            # processed.append({\n",
    "            #     'input': text,\n",
    "            #     'references': references,\n",
    "            #     'input_length': len(tokenizer.tokenize(text)),\n",
    "            #     'references_length': [len(tokenizer.tokenize(ref)) for ref in references],\n",
    "            # })\n",
    "    processed = pd.DataFrame(processed, columns=['src', 'tgt', 'src_len', 'tgt_len'])\n",
    "    return processed\n",
    "\n",
    "data_dir = '../datasets'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "processed_train = preprocess(train_dataset, tokenizer)\n",
    "processed_val = preprocess(val_dataset, tokenizer)\n",
    "processed_test = preprocess(test_dataset, tokenizer)\n",
    "processed_train.to_csv(f'{data_dir}/train.tsv', sep='\\t', index=False)\n",
    "processed_val.to_csv(f'{data_dir}/val.tsv', sep='\\t', index=False)\n",
    "processed_test.to_csv(f'{data_dir}/dev.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = \"i love acting ! i'll be famous someday . what do you do ?\"\n",
    "inputs = tokenizer(tgt, return_tensors=\"pt\", max_length=128, truncation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a light-weight model to predict the length of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src_len</th>\n",
       "      <th>tgt_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PS&gt;basketball is my favorite sport to watch.&lt;...</td>\n",
       "      <td>[i absolutely do not i am sick of politics . i...</td>\n",
       "      <td>[62, 62, 62, 62]</td>\n",
       "      <td>[15, 8, 11, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PS&gt;basketball is my favorite sport to watch.&lt;...</td>\n",
       "      <td>[beats spending the day working in the candy s...</td>\n",
       "      <td>[102, 102, 102, 102]</td>\n",
       "      <td>[18, 14, 20, 22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PS&gt;basketball is my favorite sport to watch.&lt;...</td>\n",
       "      <td>[i've been working at a candy store so i just ...</td>\n",
       "      <td>[145, 145, 145, 145]</td>\n",
       "      <td>[16, 16, 16, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PS&gt;basketball is my favorite sport to watch.&lt;...</td>\n",
       "      <td>[i've been working at a candy store so i just ...</td>\n",
       "      <td>[182, 182, 182, 182]</td>\n",
       "      <td>[16, 6, 13, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PS&gt;basketball is my favorite sport to watch.&lt;...</td>\n",
       "      <td>[good idea ! i keep my mind off things by runn...</td>\n",
       "      <td>[218, 218, 218, 218]</td>\n",
       "      <td>[12, 10, 14, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>&lt;PS&gt;zebras are my favorite animals.&lt;SEP&gt;i love...</td>\n",
       "      <td>[i love lizards , and what a great name !, Nic...</td>\n",
       "      <td>[84, 84, 84, 84]</td>\n",
       "      <td>[11, 24, 28, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5478</th>\n",
       "      <td>&lt;PS&gt;zebras are my favorite animals.&lt;SEP&gt;i love...</td>\n",
       "      <td>[nope , just my lizard gila ., I have 3. What ...</td>\n",
       "      <td>[114, 114, 114, 114]</td>\n",
       "      <td>[9, 8, 31, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5479</th>\n",
       "      <td>&lt;PS&gt;zebras are my favorite animals.&lt;SEP&gt;i love...</td>\n",
       "      <td>[how exciting . how old ? it must be interesti...</td>\n",
       "      <td>[153, 153, 153, 153]</td>\n",
       "      <td>[15, 21, 31, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5480</th>\n",
       "      <td>&lt;PS&gt;zebras are my favorite animals.&lt;SEP&gt;i love...</td>\n",
       "      <td>[that's so adorable . anything else you want t...</td>\n",
       "      <td>[206, 206, 206, 206]</td>\n",
       "      <td>[15, 4, 29, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5481</th>\n",
       "      <td>&lt;PS&gt;zebras are my favorite animals.&lt;SEP&gt;i love...</td>\n",
       "      <td>[good for you ! i do not have any . . . yet ! ...</td>\n",
       "      <td>[251, 251, 251, 251]</td>\n",
       "      <td>[17, 30, 20, 15]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5482 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    src  \\\n",
       "0     <PS>basketball is my favorite sport to watch.<...   \n",
       "1     <PS>basketball is my favorite sport to watch.<...   \n",
       "2     <PS>basketball is my favorite sport to watch.<...   \n",
       "3     <PS>basketball is my favorite sport to watch.<...   \n",
       "4     <PS>basketball is my favorite sport to watch.<...   \n",
       "...                                                 ...   \n",
       "5477  <PS>zebras are my favorite animals.<SEP>i love...   \n",
       "5478  <PS>zebras are my favorite animals.<SEP>i love...   \n",
       "5479  <PS>zebras are my favorite animals.<SEP>i love...   \n",
       "5480  <PS>zebras are my favorite animals.<SEP>i love...   \n",
       "5481  <PS>zebras are my favorite animals.<SEP>i love...   \n",
       "\n",
       "                                                    tgt               src_len  \\\n",
       "0     [i absolutely do not i am sick of politics . i...      [62, 62, 62, 62]   \n",
       "1     [beats spending the day working in the candy s...  [102, 102, 102, 102]   \n",
       "2     [i've been working at a candy store so i just ...  [145, 145, 145, 145]   \n",
       "3     [i've been working at a candy store so i just ...  [182, 182, 182, 182]   \n",
       "4     [good idea ! i keep my mind off things by runn...  [218, 218, 218, 218]   \n",
       "...                                                 ...                   ...   \n",
       "5477  [i love lizards , and what a great name !, Nic...      [84, 84, 84, 84]   \n",
       "5478  [nope , just my lizard gila ., I have 3. What ...  [114, 114, 114, 114]   \n",
       "5479  [how exciting . how old ? it must be interesti...  [153, 153, 153, 153]   \n",
       "5480  [that's so adorable . anything else you want t...  [206, 206, 206, 206]   \n",
       "5481  [good for you ! i do not have any . . . yet ! ...  [251, 251, 251, 251]   \n",
       "\n",
       "               tgt_len  \n",
       "0      [15, 8, 11, 19]  \n",
       "1     [18, 14, 20, 22]  \n",
       "2     [16, 16, 16, 17]  \n",
       "3      [16, 6, 13, 20]  \n",
       "4     [12, 10, 14, 16]  \n",
       "...                ...  \n",
       "5477  [11, 24, 28, 21]  \n",
       "5478    [9, 8, 31, 19]  \n",
       "5479  [15, 21, 31, 31]  \n",
       "5480   [15, 4, 29, 29]  \n",
       "5481  [17, 30, 20, 15]  \n",
       "\n",
       "[5482 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test.groupby('src', as_index=False).agg(list)\n",
    "# for i, row in :\n",
    "#     src, tgt, src_len, tgt_len = row['src'], row['tgt'], row['src_len'], row['tgt_len']\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seq2seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2184426/4223208514.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../DG_ckpt/bart'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blended_skill_talk'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seq2seq'"
     ]
    }
   ],
   "source": [
    "from ..seq2seq.models import EncoderRNN, DecoderRNN, Seq2seq\n",
    "\n",
    "model_path = '../../DG_ckpt/bart'\n",
    "data_n = 'blended_skill_talk'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "max_len = 128\n",
    "hidden_size = 256\n",
    "input_dropout = 0\n",
    "dropout = 0.2\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "rnn_cell = 'gru'\n",
    "attention = False\n",
    "\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "    vocab_size=tokenizer.__len__(), \n",
    "    max_len=max_len, \n",
    "    hidden_size=hidden_size,\n",
    "    input_dropout_p=input_dropout,\n",
    "    dropout_p=dropout,\n",
    "    n_layers=n_layers,\n",
    "    bidirectional=bidirectional, \n",
    "    rnn_cell=rnn_cell,\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    vocab_size=tokenizer.__len__(), \n",
    "    max_len=max_len, \n",
    "    hidden_size=hidden_size * 2 if bidirectional else hidden_size,\n",
    "    sos_id=tokenizer.bos_token_id,\n",
    "    eos_id=tokenizer.eos_token_id,\n",
    "    input_dropout_p=input_dropout,\n",
    "    dropout_p=dropout,\n",
    "    n_layers=n_layers,\n",
    "    bidirectional=bidirectional,\n",
    "    rnn_cell=rnn_cell,\n",
    "    use_attention=attention,\n",
    ")\n",
    "model = Seq2seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
