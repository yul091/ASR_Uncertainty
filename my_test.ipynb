{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torchaudio.utils import download_asset\n",
    "# from common import data_uncertainty\n",
    "# import speechbrain as sb\n",
    "# from speechbrain.pretrained import (\n",
    "#     EncoderASR,\n",
    "#     EncoderDecoderASR,\n",
    "# )\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoProcessor,\n",
    "    Speech2TextProcessor, \n",
    "    AutoModelForCTC,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    SpeechEncoderDecoderModel, # wav2vec2\n",
    "    Speech2TextForConditionalGeneration,\n",
    "    AutoModelForCTC,\n",
    "    HubertForCTC,\n",
    "    SEWForCTC,\n",
    "    SEWDForCTC,\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/monkey/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file': '/home/monkey/.cache/huggingface/datasets/downloads/extracted/882ca765a0dfdf2865144e05b76b1dc54fa85e48d74d8b8ca2d9420063365d25/dev_clean/1272/128104/1272-128104-0000.flac', 'audio': {'path': '/home/monkey/.cache/huggingface/datasets/downloads/extracted/882ca765a0dfdf2865144e05b76b1dc54fa85e48d74d8b8ca2d9420063365d25/dev_clean/1272/128104/1272-128104-0000.flac', 'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
      "       0.0010376 ], dtype=float32), 'sampling_rate': 16000}, 'text': 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL', 'speaker_id': 1272, 'chapter_id': 128104, 'id': '1272-128104-0000'}\n",
      "torch.Size([1, 584, 80])\n",
      "tensor([[   2,  129, 8053,   66,   30,    4, 5878,    8,    4, 1080, 3353,    5,\n",
      "            6,   52,   60,  534,    9, 1524,   20, 5517,    2]])\n",
      "mister quilter is the apostle of the middle classes and we are glad to welcome his gospel\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# print(model)\n",
    "print(ds[0])\n",
    "\n",
    "inputs = processor(\n",
    "    ds[0][\"audio\"][\"array\"], \n",
    "    sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], \n",
    "    return_tensors=\"pt\",\n",
    ") # input_features, attention_mask\n",
    "input_features = inputs.input_features\n",
    "print(input_features.shape)\n",
    "generated_ids = model.generate(\n",
    "    inputs=input_features,\n",
    "    max_length=128,\n",
    ")\n",
    "print(generated_ids)\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n",
    "# model = AutoModelForSpeechSeq2Seq.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n",
    "# ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# print(ds[0])\n",
    "\n",
    "# input_values = processor(\n",
    "#     ds[0][\"audio\"][\"array\"], \n",
    "#     return_tensors=\"pt\").input_values\n",
    "# print(input_values.shape)\n",
    "# # Inference: Translate English speech to German\n",
    "# generated = model.generate(input_values)\n",
    "# decoded = processor.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "# print(decoded)\n",
    "\n",
    "# # Training: Train model on English transcription\n",
    "# labels = processor(text=ds[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "# loss = model(input_values, labels=labels).loss\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "# dataset = dataset.sort(\"id\")\n",
    "# sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "# model = AutoModelForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "\n",
    "# # audio file is decoded on the fly\n",
    "# inputs = processor(\n",
    "#     dataset[0][\"audio\"][\"array\"], \n",
    "#     sampling_rate=sampling_rate, \n",
    "#     return_tensors=\"pt\")\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**inputs).logits\n",
    "# predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# # transcribe speech\n",
    "# transcription = processor.batch_decode(predicted_ids)\n",
    "# print(transcription[0])\n",
    "\n",
    "# inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "# # compute loss\n",
    "# loss = model(**inputs).loss\n",
    "# round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_1 = \"LibriSpeech/test-clean/1089/134686/1089-134686-0030.flac\"\n",
    "# model = \"asr-crdnn-rnnlm-librispeech\" \n",
    "# model = \"asr-transformer-transformerlm-librispeech\"\n",
    "# model = \"asr-transformer-transformerlm-librispeech\"\n",
    "# model = \"asr-wav2vec2-librispeech\"\n",
    "model = \"asr-wav2vec2-commonvoice-fr\"\n",
    "\n",
    "# # Uncomment for using another pre-trained model\n",
    "# asr_model = EncoderASR.from_hparams(\n",
    "#     source=f\"speechbrain/{model}\", \n",
    "#     savedir=f\"pretrained_models/{model}\", \n",
    "#     # run_opts={\"device\":\"cuda\"}, # inference on GPU\n",
    "#     hparams_file=\"hyperparams.yaml\",\n",
    "# )\n",
    "# asr_model.transcribe_file(audio_1)\n",
    "\n",
    "asr_model = EncoderASR.from_hparams(\n",
    "    source=\"speechbrain/asr-wav2vec2-commonvoice-fr\",\n",
    "    savedir=f\"pretrained_models/{model}\",\n",
    ") \n",
    "asr_model.transcribe_file(\"samples/audio_samples/example_fr.wav\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asr_model (EncoderDecoderASR)\n",
    "# mods: ModuleDict(InputNormalization, Encoder, Decoder, LM)\n",
    "    # Encoder: LengthsCapableSequential\n",
    "    # Decoder: S2SRNNBeamSearchTransformerLM\n",
    "    # LM: TransformerLM\n",
    "from speechbrain.nnet.containers import LengthsCapableSequential\n",
    "from speechbrain.decoders import S2SRNNBeamSearchLM, S2SRNNBeamSearchTransformerLM\n",
    "from speechbrain.lobes.models.transformer.TransformerLM import TransformerLM\n",
    "\n",
    "return_log_probs = True\n",
    "emb = torch.nn.Embedding(5, 3)\n",
    "dec = sb.nnet.RNN.AttentionalRNNDecoder(\n",
    "    \"gru\", \"content\", 3, 3, 1, enc_dim=7, input_size=3\n",
    ")\n",
    "lin = sb.nnet.linear.Linear(n_neurons=5, input_size=3)\n",
    "lm = TransformerLM(5, 512, 8, 1, 0, 1024, activation=torch.nn.GELU)\n",
    "searcher = S2SRNNBeamSearchTransformerLM(\n",
    "    embedding=emb,\n",
    "    decoder=dec,\n",
    "    linear=lin,\n",
    "    language_model=lm,\n",
    "    bos_index=4,\n",
    "    eos_index=4,\n",
    "    blank_index=4,\n",
    "    min_decode_ratio=0,\n",
    "    max_decode_ratio=1,\n",
    "    beam_size=2,\n",
    "    lm_weight=0.5,\n",
    "    return_log_probs=return_log_probs,\n",
    ")\n",
    "enc = torch.rand([2, 6, 7]) # B X T X D\n",
    "wav_len = torch.rand([2]) # B\n",
    "outputs = searcher(enc, wav_len)\n",
    "if return_log_probs:\n",
    "    predictions, topk_scores, log_probs = outputs\n",
    "    print(\"log-probs: \", log_probs)\n",
    "else:\n",
    "    predictions, topk_scores = outputs\n",
    "print(\"predictions: \", predictions)\n",
    "print(\"topk_scores: \", topk_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_1 = \"LibriSpeech/test-clean/1089/134686/1089-134686-0029.flac\"\n",
    "waveform = asr_model.load_audio(audio_1)\n",
    "# Fake a batch\n",
    "batch = waveform.unsqueeze(0)\n",
    "print('wave tensor: ', batch.shape)\n",
    "rel_length = torch.tensor([1.0])\n",
    "# predicted_words, predicted_tokens = asr_model.transcribe_batch(batch, rel_length)\n",
    "with torch.no_grad():\n",
    "    wav_lens = rel_length\n",
    "    encoder_out = asr_model.encode_batch(batch, wav_lens) # B X T X D\n",
    "    print(encoder_out.shape)\n",
    "    predicted_tokens, scores, log_probs = asr_model.mods.decoder(encoder_out, wav_lens)\n",
    "    print([len(tokens) for tokens in predicted_tokens])\n",
    "    print(\"scores: \", scores.shape)\n",
    "    print(\"log_probs: \", [log_prob.shape for log_prob in log_probs])\n",
    "    predicted_words = [\n",
    "        asr_model.tokenizer.decode_ids(token_seq)\n",
    "        for token_seq in predicted_tokens\n",
    "    ]\n",
    "    print(predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1 = 'BEWARE OF MAKING THAT MISTAKE'\n",
    "preds2 = 'GIVE NOT SO EARNEST A MIND TO THESE MUMMERIES CHILD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.tokenizer.decode_ids([224, 336, 11, 53, 90, 32, 66, 49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "f1 = open(\"LibriSpeech-asr-crdnn-rnnlm-librispeech-energy.log\", \"r\")\n",
    "f2 = open(\"LibriSpeech-asr-crdnn-transformerlm-librispeech-energy.log\", \"r\")\n",
    "\n",
    "def get_XY(f):\n",
    "    unit = \"mW\"\n",
    "    inss, avgs = [], []\n",
    "    for line in f:\n",
    "        r = re.compile('VDD_GPU_SOC (.+?)([ \\.]|$)')\n",
    "        patterns = re.findall(r, line)\n",
    "        if not patterns:\n",
    "            continue\n",
    "        GPU_pair = patterns[0][0]\n",
    "        ins, avg = GPU_pair.split('/')\n",
    "        ins = int(ins.rstrip(unit))\n",
    "        inss.append(ins)\n",
    "        avg = int(avg.rstrip(unit))\n",
    "        avgs.append(avg)\n",
    "    return inss, avgs\n",
    "\n",
    "inss1, avgs1 = get_XY(f1)\n",
    "inss2, avgs2 = get_XY(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx2 = np.arange(len(inss2))\n",
    "yy2 = np.array(inss2)\n",
    "plt.plot(xx2, yy2, label='CRDNN + TransLM')\n",
    "plt.fill_between(xx2, yy2, y2 = min(yy2), alpha=0.3,)\n",
    "xx1 = np.arange(len(inss1))\n",
    "yy1 = np.array(inss1)\n",
    "plt.plot(xx1, yy1, label='CRDNN + RNNLM')\n",
    "plt.fill_between(xx1, yy1, y2 = min(yy1), alpha=0.3,)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'Energy': [min(inss2[2:]), min(inss1[2:]), np.mean(inss2[1:]), np.mean(inss1[1:]), max(inss2[1:]), max(inss1[1:])],\n",
    "        'Model': ['CRDNN + TransLM', 'CRDNN + RNNLM']*3,\n",
    "    }\n",
    ")\n",
    "print(df)\n",
    "f = plt.figure(figsize=(4.5, 6))\n",
    "ax = sns.boxplot(x='Model', y='Energy', data=df, hue='Model', dodge=False, width=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_WAV = \"LibriSpeech/test-clean/1089/134686/1089-134686-0030.flac\"\n",
    "# SAMPLE_WAV = download_asset(\"tutorial-assets/steam-train-whistle-daniel_simon.wav\")\n",
    "print(SAMPLE_WAV)\n",
    "waveform = asr_model.load_audio(SAMPLE_WAV).unsqueeze(0)\n",
    "\n",
    "print(waveform)\n",
    "plot_waveform(waveform, 16000)\n",
    "Audio(waveform, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NOISE = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo-8000hz.wav\")\n",
    "# noise, noise_sample_rate = torchaudio.load(SAMPLE_NOISE)\n",
    "noise = asr_model.load_audio(SAMPLE_NOISE).unsqueeze(0)\n",
    "print(noise.shape)\n",
    "plot_waveform(noise, 16000)\n",
    "Audio(noise, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "# Handle noise\n",
    "if noise.shape[1] < waveform.shape[1]:\n",
    "    K = ceil(waveform.shape[1] / noise.shape[1])\n",
    "    noi = noise.repeat(1, K)[:, :waveform.shape[1]]\n",
    "else:\n",
    "    noi = noise[:, :waveform.shape[1]]\n",
    "\n",
    "print(waveform.shape, noi.shape)\n",
    "snr_dbs = torch.tensor([20, 10, 3])\n",
    "noisy_speech = add_noise(waveform, noi, snr_dbs)\n",
    "print(noisy_speech.shape)\n",
    "plot_waveform(noisy_speech.mean(dim=0).unsqueeze(0), 16000)\n",
    "Audio(noisy_speech.mean(dim=0).unsqueeze(0), rate=16000)\n",
    "# noisy_speech = waveform + noi\n",
    "# print(noisy_speech)\n",
    "# plot_waveform(noisy_speech, 16000)\n",
    "# Audio(noisy_speech, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "def add_noise(\n",
    "    waveform: torch.Tensor, noise: torch.Tensor, snr: torch.Tensor, lengths: Optional[torch.Tensor] = None\n",
    ") -> torch.Tensor:\n",
    "    if not (waveform.ndim - 1 == noise.ndim - 1 == snr.ndim and (lengths is None or lengths.ndim == snr.ndim)):\n",
    "        raise ValueError(\"Input leading dimensions don't match.\")\n",
    "\n",
    "    L = waveform.size(-1)\n",
    "\n",
    "    if L != noise.size(-1):\n",
    "        raise ValueError(f\"Length dimensions of waveform and noise don't match (got {L} and {noise.size(-1)}).\")\n",
    "\n",
    "    # compute scale\n",
    "    if lengths is not None:\n",
    "        mask = torch.arange(0, L, device=lengths.device).expand(waveform.shape) < lengths.unsqueeze(\n",
    "            -1\n",
    "        )  # (*, L) < (*, 1) = (*, L)\n",
    "        masked_waveform = waveform * mask\n",
    "        masked_noise = noise * mask\n",
    "    else:\n",
    "        masked_waveform = waveform\n",
    "        masked_noise = noise\n",
    "\n",
    "    energy_signal = torch.linalg.vector_norm(masked_waveform, ord=2, dim=-1) ** 2  # (*,)\n",
    "    energy_noise = torch.linalg.vector_norm(masked_noise, ord=2, dim=-1) ** 2  # (*,)\n",
    "    original_snr_db = 10 * (torch.log10(energy_signal) - torch.log10(energy_noise))\n",
    "    scale = 10 ** ((original_snr_db - snr) / 20.0)  # (*,)\n",
    "\n",
    "    # scale noise\n",
    "    scaled_noise = scale.unsqueeze(-1) * noise  # (*, 1) * (*, L) = (*, L)\n",
    "\n",
    "    return waveform + scaled_noise  # (*, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE_NOISE = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo-8000hz.wav\")\n",
    "# noise, noise_sample_rate = torchaudio.load(SAMPLE_NOISE)\n",
    "\n",
    "SAMPLE_WAV = \"LibriSpeech/test-clean/1089/134686/1089-134686-0030.flac\"\n",
    "# SAMPLE_WAV = download_asset(\"tutorial-assets/steam-train-whistle-daniel_simon.wav\")\n",
    "print(SAMPLE_WAV)\n",
    "waveform = asr_model.load_audio(SAMPLE_WAV)\n",
    "# Fake a batch\n",
    "batch = waveform.unsqueeze(0)\n",
    "print('wave tensor: ', batch.shape)\n",
    "# Add noise\n",
    "noise = torch.tensor(np.random.normal(0, 0.5, 100000).reshape(1, -1))\n",
    "noise = noise[:, :waveform.shape[0]]\n",
    "snr_dbs = torch.tensor([20, 10, 3])\n",
    "noisy_speech = add_noise(batch, noise, snr_dbs)\n",
    "rel_length = torch.tensor([1.0])\n",
    "\n",
    "predicted_words, predicted_tokens = asr_model.transcribe_batch(batch, rel_length)\n",
    "print(predicted_words)\n",
    "predicted_words, predicted_tokens = asr_model.transcribe_batch(noisy_speech[1:2], rel_length)\n",
    "print(predicted_words)\n",
    "# with torch.no_grad():\n",
    "#     wav_lens = rel_length\n",
    "#     encoder_out = asr_model.encode_batch(batch, wav_lens) # B X T X D\n",
    "#     print(encoder_out.shape)\n",
    "#     predicted_tokens, scores = asr_model.mods.decoder(encoder_out, wav_lens)\n",
    "#     print(predicted_tokens)\n",
    "#     predicted_words = [\n",
    "#         asr_model.tokenizer.decode_ids(token_seq)\n",
    "#         for token_seq in predicted_tokens\n",
    "#     ]\n",
    "#     print(predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# noisy_speech.shape\n",
    "Audio(noise, rate=noise_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(SAMPLE_WAV)\n",
    "\n",
    "Audio(waveform, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(noisy_speech, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NOISE = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo-8000hz.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# SAMPLE_RATE = 44100\n",
    "# BASE_DIR = Path(os.path.abspath(os.path.dirname(os.path.dirname(__file__))))\n",
    "# SCRIPTS_DIR = BASE_DIR / \"scripts\"\n",
    "# TEST_FIXTURES_DIR = BASE_DIR / \"test_fixtures\"\n",
    "\n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion, AddBackgroundNoise\n",
    "from torch_audiomentations.utils import TEST_FIXTURES_DIR\n",
    "\n",
    "# Initialize augmentation callable\n",
    "apply_augmentation = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5),\n",
    "        AddBackgroundNoise(\n",
    "            background_paths=TEST_FIXTURES_DIR / \"bg\", \n",
    "            p=0.5,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "perturbed_audio_samples = apply_augmentation(batch.unsqueeze(0), sample_rate=16000)\n",
    "\n",
    "# modes = [\"per_batch\", \"per_example\", \"per_channel\"]\n",
    "# for mode in modes:\n",
    "#     transform = {\n",
    "#         \"get_instance\": lambda: AddBackgroundNoise(\n",
    "#             background_paths=TEST_FIXTURES_DIR / \"bg\", mode=mode, p=1.0\n",
    "#         ),\n",
    "#         \"num_runs\": 5,\n",
    "#     }\n",
    "#     perturbed_audio_samples = transform(batch.unsqueeze(0), sample_rate=SAMPLE_RATE)\n",
    "    \n",
    "\n",
    "# transform = AddBackgroundNoise(\n",
    "#     os.path.join(tempfile.gettempdir(), str(uuid.uuid4())),\n",
    "#     min_snr_in_db=4,\n",
    "#     max_snr_in_db=6,\n",
    "#     p=1.0,\n",
    "#     sample_rate=16000,\n",
    "#     output_type=\"dict\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_audio_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(perturbed_audio_samples.squeeze(0), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(batch, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None):\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-define gaussian noise\n",
    "import numpy as np\n",
    "\n",
    "# Adding noise using a target noise power\n",
    "\n",
    "# Set a target channel noise power to something very noisy\n",
    "target_noise_db = 10\n",
    "\n",
    "# Convert to linear Watt units\n",
    "target_noise_watts = 10 ** (target_noise_db / 10)\n",
    "\n",
    "# Generate noise samples\n",
    "mean_noise = 0\n",
    "noise_volts = np.random.normal(mean_noise, np.sqrt(target_noise_watts), len(x_watts))\n",
    "\n",
    "# Noise up the original signal (again) and plot\n",
    "y_volts = x_volts + noise_volts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noise = np.random.normal(0, 0.1, 80000).reshape(1, -1)\n",
    "# plt.plot(noise)\n",
    "plot_waveform(noise, noise_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ed07da37284b78ddf1027a77eacdaaa03fe44cd85ed96169aaea4ffb3c093db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
